# Stream-Agent 项目面试备战文档

> 本文档用于面试准备，包含项目介绍话术、技术架构详解、核心模块分析等内容。

---

## 目录

1. [简历项目介绍（书面版）](#一简历项目介绍书面版)
2. [面试口述话术（1分钟/3分钟/5分钟版）](#二面试口述话术)
3. [技术架构详解](#三技术架构详解)
4. [核心模块详解](#四核心模块详解)
   - 4.1 认证服务 (auth-service)
   - 4.2 聊天服务 (chat-service)
   - 4.3 RAG 服务 (rag-service)
   - 4.4 语音服务 (whisper-service)
5. [技术亮点与难点](#五技术亮点与难点)
6. [面试常见问题与回答](#六面试常见问题与回答)
7. [技术选型理由](#七技术选型理由)

---

## 一、简历项目介绍（书面版）

### 简洁版（适合简历）

**Stream-Agent - 全栈 AI 研究助理平台**

- 基于 LangChain + LangGraph 构建的多工具 Agent 系统，支持 96+ 工具调用
- 采用微服务架构，包含认证、聊天、RAG、语音四大核心服务
- 实现 RAG 混合检索（向量检索 + BM25 + Reranker 重排序），支持引用追溯
- 集成 E2B 云沙箱实现安全的代码执行和数据可视化
- 支持多模态交互：图片理解、语音输入（Whisper）、语音合成（Edge TTS）
- 前端采用 Next.js 14 + SSE 流式响应，实现打字机效果的实时对话

**技术栈**: Next.js 14, FastAPI, LangGraph, PostgreSQL + pgvector, Milvus, Docker

---

### 详细版（适合项目经历描述）

**Stream-Agent - 全栈 AI 研究助理平台** | 个人项目 | 2024.12 - 至今

**项目描述**:
一个面向研究人员的 AI 助理平台，集成了多工具 Agent、知识库检索、代码执行、多模态交互等功能，帮助用户进行文献调研、数据分析、代码编写等研究工作。

**核心职责**:
- 设计并实现微服务架构，将系统拆分为认证、聊天、RAG、语音四个独立服务
- 基于 LangGraph 实现 ReAct Agent，支持 96+ 工具的动态调用和流式响应
- 设计 RAG 混合检索方案，融合向量检索（pgvector）、关键词检索（BM25）和 Reranker 重排序
- 实现 SSE 流式传输，支持工具调用可视化、代码高亮、图表渲染等实时反馈
- 集成 E2B 云沙箱，实现安全隔离的 Python 代码执行和数据可视化
- 开发语音交互模块，集成 faster-whisper 语音识别和 Edge TTS 语音合成

**技术亮点**:
- 采用 RRF（Reciprocal Rank Fusion）算法融合多路检索结果，提升检索准确率
- 实现智能分块策略（语义感知/页面感知/递归分块），优化长文档处理
- 设计引用追溯机制，支持 RAG 结果的来源定位和原文高亮
- 使用 MCP（Model Context Protocol）协议集成 90+ 外部工具

**技术栈**:
- 前端: Next.js 14, React 18, TypeScript, Tailwind CSS, shadcn/ui, Zustand
- 后端: FastAPI, LangGraph, LangChain, SQLAlchemy, Pydantic
- 数据库: PostgreSQL + pgvector, SQLite（测试）
- AI/ML: Google Gemini, faster-whisper, Edge TTS, E2B Sandbox
- 部署: Docker, Render, Supabase

---

## 二、面试口述话术

### 1 分钟版

> "我做的是一个全栈 AI 研究助理平台，叫 Stream-Agent。
>
> 它的核心是一个基于 LangGraph 的多工具 Agent 系统，可以调用 96 种工具来帮助用户完成各种任务，比如网络搜索、代码执行、文献检索等。
>
> 技术上，我采用了微服务架构，分为认证、聊天、RAG、语音四个服务。其中 RAG 模块是一个亮点，我实现了向量检索加 BM25 加 Reranker 的混合检索方案，并且支持引用追溯。
>
> 前端用的是 Next.js 14，通过 SSE 实现流式响应，用户可以实时看到 AI 的思考过程和工具调用结果。
>
> 整个项目让我深入理解了 LLM 应用开发、RAG 系统设计和微服务架构。"

### 3 分钟版

> "我来详细介绍一下我的项目 Stream-Agent。
>
> **项目背景**: 这是一个面向研究人员的 AI 助理平台。我发现现有的 AI 工具要么功能单一，要么不支持本地知识库，所以我决定做一个集成多种能力的研究助理。
>
> **系统架构**: 我采用了微服务架构，分为四个核心服务：
> - 认证服务负责用户管理和 JWT 鉴权
> - 聊天服务是核心，基于 LangGraph 实现了 ReAct Agent
> - RAG 服务处理文档解析和混合检索
> - 语音服务支持语音输入和合成
>
> **技术亮点**:
>
> 第一个亮点是 Agent 系统。我用 LangGraph 实现了一个支持 96 种工具的 Agent，包括网络搜索、代码执行、学术论文检索等。工具调用采用 MCP 协议，可以动态扩展。
>
> 第二个亮点是 RAG 混合检索。我没有只用向量检索，而是融合了三种方法：pgvector 做向量检索、BM25 做关键词检索、然后用 Reranker 重排序。用 RRF 算法融合结果，实测比单一方法准确率提升了 30%。
>
> 第三个亮点是流式响应。前端通过 SSE 接收后端的流式数据，可以实时显示 AI 的思考过程、工具调用状态、代码执行结果。用户体验很好。
>
> **遇到的挑战**: 最大的挑战是 RAG 的检索质量。一开始用纯向量检索，发现对专业术语效果不好。后来我加入了 BM25 和 Reranker，并且优化了分块策略，才达到满意的效果。
>
> 这个项目让我对 LLM 应用开发有了很深的理解，特别是 Agent 设计和 RAG 优化方面。"

### 5 分钟版

（在 3 分钟版基础上，可以展开以下内容）

> **更多技术细节**:
>
> 关于 Agent 实现，我用的是 LangGraph 的 StateGraph。每个节点代表一个处理步骤：接收用户输入、调用 LLM 决策、执行工具、生成回复。状态在节点间流转，支持复杂的多轮对话和工具链调用。
>
> 关于 RAG 系统，我实现了完整的流程：
> 1. 文档解析：支持 PDF、Word、Markdown，用 PyPDF2 和 MinerU 提取内容
> 2. 智能分块：根据文档类型选择分块策略，比如学术论文用页面感知分块
> 3. 向量化：用 text-embedding-3-small 生成向量，存入 pgvector
> 4. 混合检索：同时查询向量库和 BM25 索引，用 RRF 融合
> 5. 重排序：用 bge-reranker 对结果重排序
> 6. 引用追溯：记录每个 chunk 的来源，支持原文定位
>
> 关于代码执行，我集成了 E2B 云沙箱。用户可以让 AI 写代码并执行，结果会实时返回。沙箱是隔离的，不会影响服务器安全。
>
> **项目收获**:
> 1. 深入理解了 LLM 应用的架构设计
> 2. 掌握了 RAG 系统的优化方法
> 3. 积累了微服务开发和部署经验
> 4. 提升了全栈开发能力

---

## 三、技术架构详解

### 3.1 整体架构图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        Stream-Agent V9.0                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Frontend (Next.js 14 + shadcn/ui + Tailwind)     Port: 3000            │
│  - SSE 流式接收                                                          │
│  - 工具调用可视化                                                         │
│  - 代码高亮 + 图表渲染                                                    │
│                            │                                            │
│                     HTTP/REST + SSE                                     │
│                            │                                            │
│  ┌─────────────────────────┼─────────────────────────────┐              │
│  │                         │                             │              │
│  ▼                         ▼                             ▼              │
│  auth-service         chat-service                 rag-service          │
│  (FastAPI)            (FastAPI)                    (FastAPI)            │
│  Port: 8001           Port: 8002                   Port: 8004           │
│  ┌──────────────┐     ┌──────────────────┐        ┌──────────────────┐  │
│  │ JWT 认证     │     │ LangGraph Agent  │        │ 文档解析         │  │
│  │ 用户管理     │     │ 96+ 工具         │        │ 混合检索         │  │
│  │ 权限验证     │     │ 流式响应         │        │ 引用追溯         │  │
│  └──────────────┘     │ 会话管理         │        │ Reranker         │  │
│                       └──────────────────┘        └──────────────────┘  │
│                            │                             │              │
│                            ▼                             ▼              │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                        Data Layer                                │    │
│  │  PostgreSQL (用户/会话)  │  pgvector (向量检索)  │  BM25 (关键词) │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │  External Services                                               │    │
│  │  Google Gemini │ E2B Sandbox │ MCP Tools (90+) │ Edge TTS       │    │
│  └─────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────┘
```

### 3.2 服务划分

| 服务 | 端口 | 职责 | 技术栈 |
|------|------|------|--------|
| frontend-next | 3000 | 用户界面、SSE 接收 | Next.js 14, React 18, Zustand |
| auth-service | 8001 | 用户认证、JWT 管理 | FastAPI, SQLAlchemy, bcrypt |
| chat-service | 8002 | Agent 核心、工具调用 | FastAPI, LangGraph, MCP |
| rag-service | 8004 | 文档处理、混合检索 | FastAPI, pgvector, BM25 |
| whisper-service | 8003 | 语音识别、语音合成 | FastAPI, faster-whisper, Edge TTS |

### 3.3 数据流

```
用户输入 → 前端 → chat-service → LangGraph Agent
                                      │
                    ┌─────────────────┼─────────────────┐
                    ▼                 ▼                 ▼
              工具调用            RAG 检索          LLM 生成
              (MCP/E2B)      (rag-service)       (Gemini)
                    │                 │                 │
                    └─────────────────┼─────────────────┘
                                      ▼
                              SSE 流式响应 → 前端渲染
```

---

## 四、核心模块详解

> 以下内容将分步骤扩充，每个模块包含：架构设计、核心代码、技术要点、面试问答

### 4.1 认证服务 (auth-service)

#### 架构设计

```
auth-service/
├── app/
│   ├── api/v1/
│   │   └── auth.py          # 认证路由 (register/login/refresh/logout/verify)
│   ├── core/
│   │   ├── security.py      # JWT 生成/验证、密码哈希
│   │   └── deps.py          # 依赖注入 (get_db, get_current_user)
│   ├── models/
│   │   └── user.py          # SQLAlchemy 用户模型
│   ├── schemas/
│   │   ├── user.py          # Pydantic 用户 Schema
│   │   └── token.py         # Token Schema
│   └── services/
│       └── user_service.py  # 用户业务逻辑
```

#### 核心功能

| API 端点 | 方法 | 功能 | 认证要求 |
|----------|------|------|----------|
| `/auth/register` | POST | 用户注册 | 无 |
| `/auth/login` | POST | 用户登录，返回 Token | 无 |
| `/auth/refresh` | POST | 刷新 Access Token | 需要 Refresh Token |
| `/auth/logout` | POST | 登出，撤销 Refresh Token | 需要认证 |
| `/auth/me` | GET | 获取当前用户信息 | 需要认证 |
| `/auth/verify` | GET | 验证 Token（供其他服务调用） | 需要 Token |

#### 技术实现要点

**1. 密码加密 (bcrypt)**

```python
# 使用 bcrypt 进行密码哈希，自动加盐
def get_password_hash(password: str) -> str:
    salt = bcrypt.gensalt()
    hashed = bcrypt.hashpw(password.encode('utf-8'), salt)
    return hashed.decode('utf-8')

def verify_password(plain_password: str, hashed_password: str) -> bool:
    return bcrypt.checkpw(
        plain_password.encode('utf-8'),
        hashed_password.encode('utf-8')
    )
```

**2. JWT Token 设计**

```python
# Access Token 结构
{
    "sub": "user_id",           # 用户 ID
    "exp": 1234567890,          # 过期时间
    "type": "access",           # Token 类型
    "iat": 1234567890,          # 签发时间
    "username": "xxx",          # 用户名
    "email": "xxx@xxx.com"      # 邮箱
}

# Refresh Token 结构 (增加 jti 防止重放)
{
    "sub": "user_id",
    "exp": 1234567890,
    "type": "refresh",
    "iat": 1234567890,
    "jti": "uuid"               # 唯一标识符，确保每个 token 不同
}
```

**3. Token 刷新机制**

- Access Token 有效期短（30 分钟），用于 API 认证
- Refresh Token 有效期长（7 天），用于获取新的 Access Token
- Refresh Token 使用 jti（JWT ID）确保唯一性，防止重放攻击
- 登出时撤销 Refresh Token

#### 面试问答

**Q: 为什么使用 bcrypt 而不是 MD5/SHA256？**

> bcrypt 专为密码哈希设计，有以下优势：
> 1. 自动加盐，防止彩虹表攻击
> 2. 计算成本可调（work factor），可以随硬件升级增加难度
> 3. 故意设计为慢速，增加暴力破解成本
>
> MD5/SHA256 是通用哈希算法，速度快，不适合密码存储。

**Q: 为什么要分 Access Token 和 Refresh Token？**

> 1. **安全性**: Access Token 有效期短，即使泄露影响有限
> 2. **用户体验**: Refresh Token 有效期长，用户不需要频繁登录
> 3. **灵活性**: 可以单独撤销 Refresh Token，不影响其他设备
> 4. **性能**: Access Token 验证不需要查数据库，Refresh Token 刷新时才查

**Q: 如何防止 JWT 被盗用？**

> 1. 使用 HTTPS 传输
> 2. Access Token 设置短有效期
> 3. Refresh Token 存储在 HttpOnly Cookie 中
> 4. 实现 Token 黑名单机制
> 5. 使用 jti 防止重放攻击

### 4.2 聊天服务 (chat-service)

#### 架构设计

```
chat-service/
├── app/
│   ├── api/v1/
│   │   ├── chat.py           # 聊天 API (SSE 流式响应)
│   │   └── conversations.py  # 会话管理 API
│   ├── services/
│   │   └── agent_service.py  # LangGraph Agent 核心
│   ├── models/
│   │   └── conversation.py   # 会话模型
│   └── schemas/
│       └── chat.py           # 请求/响应 Schema
└── tools/                    # 共享工具目录
    ├── rag_search_tool.py    # RAG 检索工具
    ├── e2b_tools.py          # E2B 代码执行工具
    └── structure_tools.py    # 结构化输出工具
```

#### LangGraph Agent 架构

```
┌─────────────────────────────────────────────────────────────┐
│                    LangGraph ReAct Agent                     │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐  │
│  │  Input  │ →  │   LLM   │ →  │  Tool   │ →  │ Output  │  │
│  │ Message │    │Decision │    │Execution│    │Response │  │
│  └─────────┘    └─────────┘    └─────────┘    └─────────┘  │
│       ↑              │              │              │        │
│       │              ▼              ▼              │        │
│       │    ┌─────────────────────────────┐        │        │
│       │    │      State (Checkpointer)   │        │        │
│       │    │   - 会话历史                 │        │        │
│       │    │   - 工具调用记录             │        │        │
│       └────│   - 用户隔离 (thread_id)    │────────┘        │
│            └─────────────────────────────┘                  │
│                                                             │
│  Tools (96+):                                               │
│  ┌──────────────┬──────────────┬──────────────┐            │
│  │ MCP Tools    │ Custom Tools │ E2B Tools    │            │
│  │ (~90)        │ (4)          │ (6)          │            │
│  │ - 网络搜索   │ - RAG 检索   │ - 代码执行   │            │
│  │ - 电商数据   │ - 论文格式化 │ - CSV 分析   │            │
│  │ - 社交媒体   │              │ - 图表生成   │            │
│  └──────────────┴──────────────┴──────────────┘            │
└─────────────────────────────────────────────────────────────┘
```

#### 核心代码解析

**1. Agent 初始化**

```python
async def initialize_agent(user_id: str, api_keys: Dict[str, str] = None):
    """为每个用户创建独立的 Agent 实例"""

    # 1. 加载工具 (MCP + 自定义 + E2B)
    all_tools = await get_tools(api_keys)

    # 2. 配置 LLM (支持 Gemini 和 OpenAI 兼容)
    if llm_provider == "openai_compatible":
        llm = ChatOpenAI(model=openai_model, base_url=openai_base_url, ...)
    else:
        llm = ChatGoogleGenerativeAI(model=google_model, ...)

    # 3. 创建状态持久化 (SQLite Checkpointer)
    checkpointer = AsyncSqliteSaver(sqlite_conn)

    # 4. 创建 ReAct Agent
    agent_executor = create_react_agent(
        model=llm,
        tools=all_tools,
        checkpointer=checkpointer,
    )

    return agent_executor
```

**2. SSE 流式响应**

```python
async def chat_with_agent_stream(...) -> AsyncGenerator[str, None]:
    """流式返回 Agent 响应"""

    # 构建 thread_id 实现用户隔离
    thread_id = f"{user_id}:{conversation_id}"
    config = {"configurable": {"thread_id": thread_id}}

    # 流式处理事件
    async for event in agent.astream_events({"messages": messages}, config=config):
        kind = event["event"]

        if kind == "on_chat_model_stream":
            # 文本流式输出
            yield f"event: text\ndata: {encoded_data}\n\n"

        elif kind == "on_tool_start":
            # 工具开始调用
            yield f"event: tool_start\ndata: {tool_name}\n\n"

        elif kind == "on_tool_end":
            # 工具调用完成，返回结果
            yield f"event: tool_end\ndata: {result}\n\n"
```

**3. 多模态支持**

```python
def build_multimodal_content(text: str, image_list: List[str] = None):
    """构建多模态消息内容"""
    content_parts = []

    # 添加图片 (Vision 模型期望图片在文本之前)
    if image_list:
        for img_base64 in image_list:
            content_parts.append({
                "type": "image_url",
                "image_url": {"url": f"data:{media_type};base64,{img_base64}"}
            })

    # 添加文本
    content_parts.append({"type": "text", "text": text})

    return content_parts
```

#### 工具系统设计

| 工具类型 | 数量 | 来源 | 示例 |
|----------|------|------|------|
| **MCP 工具** | ~90 | BrightData MCP | search_engine, scrape_as_markdown |
| **RAG 工具** | 2 | 自定义 | rag_search, list_knowledge_documents |
| **E2B 工具** | 6 | 自定义 | execute_python_code, analyze_csv_data |
| **结构化工具** | 2 | 自定义 | format_paper_analysis, format_linkedin_profile |

#### 面试问答

**Q: 为什么选择 LangGraph 而不是 LangChain Agent?**

> 1. **状态管理**: LangGraph 提供了更好的状态管理机制，支持 Checkpointer 持久化
> 2. **流式支持**: 原生支持 `astream_events`，可以精细控制流式输出
> 3. **可扩展性**: 基于图的架构，可以轻松添加自定义节点和边
> 4. **调试友好**: 可以追踪每个节点的执行状态

**Q: 如何实现用户隔离?**

> 使用 `thread_id = f"{user_id}:{conversation_id}"` 作为状态键：
> 1. 每个用户的会话状态独立存储
> 2. Checkpointer 按 thread_id 隔离数据
> 3. Agent 实例按 user_id 缓存，避免重复初始化

**Q: SSE 和 WebSocket 的区别?**

> | 特性 | SSE | WebSocket |
> |------|-----|-----------|
> | 方向 | 单向（服务器→客户端） | 双向 |
> | 协议 | HTTP | 独立协议 |
> | 重连 | 自动重连 | 需要手动处理 |
> | 复杂度 | 简单 | 复杂 |
>
> 选择 SSE 的原因：聊天场景主要是服务器推送，SSE 更简单且自动重连。

**Q: 如何处理工具调用超时?**

> 1. 设置工具级别的超时时间
> 2. 使用 asyncio.wait_for 包装工具调用
> 3. 超时后返回友好的错误信息
> 4. E2B 沙箱有独立的超时机制

### 4.3 RAG 服务 (rag-service)

#### 架构设计

```
rag-service/
├── app/
│   ├── api/v1/
│   │   ├── documents.py      # 文档上传/管理 API
│   │   └── search.py         # 检索 API
│   ├── services/
│   │   ├── pgvector_service.py   # 向量存储 (pgvector/SQLite)
│   │   ├── bm25_service.py       # BM25 关键词检索
│   │   ├── search_service.py     # 混合检索 + RRF 融合
│   │   ├── embedding_service.py  # 向量化服务
│   │   ├── chunking_service.py   # 智能分块
│   │   ├── rerank_service.py     # Reranker 重排序
│   │   └── citation_service.py   # 引用追溯
│   └── models/
│       ├── document.py       # 文档模型
│       └── chunk.py          # 分块模型
```

#### 混合检索架构

```
┌─────────────────────────────────────────────────────────────────┐
│                      RAG 混合检索流程                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  用户查询                                                        │
│      │                                                          │
│      ▼                                                          │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                    Query Processing                      │    │
│  │  1. 中英文分词 (jieba)                                    │    │
│  │  2. 向量化 (text-embedding-3-small)                      │    │
│  └─────────────────────────────────────────────────────────┘    │
│      │                                                          │
│      ├──────────────────┬──────────────────┐                    │
│      ▼                  ▼                  ▼                    │
│  ┌────────────┐    ┌────────────┐    ┌────────────┐            │
│  │  向量检索   │    │  BM25检索  │    │  (可选)    │            │
│  │  pgvector  │    │  关键词    │    │  全文检索  │            │
│  └────────────┘    └────────────┘    └────────────┘            │
│      │                  │                  │                    │
│      └──────────────────┼──────────────────┘                    │
│                         ▼                                       │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │              RRF (Reciprocal Rank Fusion)                │    │
│  │  score = α/(k+rank_vector) + (1-α)/(k+rank_bm25)        │    │
│  └─────────────────────────────────────────────────────────┘    │
│                         │                                       │
│                         ▼                                       │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                  Reranker 重排序                          │    │
│  │  使用 bge-reranker 对融合结果重新排序                      │    │
│  └─────────────────────────────────────────────────────────┘    │
│                         │                                       │
│                         ▼                                       │
│                    返回 Top-K 结果                               │
└─────────────────────────────────────────────────────────────────┘
```

#### 核心代码解析

**1. RRF 融合算法**

```python
def rrf_fusion(
    self,
    vector_results: List[VectorSearchResult],
    bm25_results: List[BM25Result],
    alpha: float = 0.5,
    k: int = 60
) -> List[FusedResult]:
    """
    Reciprocal Rank Fusion (RRF) 融合算法

    公式: RRF(d) = sum(1 / (k + r_i)) for each ranking r_i
    """
    scores: Dict[str, Dict[str, Any]] = {}

    # 处理向量检索结果
    for rank, result in enumerate(vector_results):
        chunk_id = result.id
        rrf_score = alpha / (k + rank + 1)
        scores[chunk_id] = {
            "fused_score": rrf_score,
            "vector_score": result.score,
            ...
        }

    # 处理 BM25 检索结果
    for rank, result in enumerate(bm25_results):
        chunk_id = result.chunk_id
        rrf_score = (1 - alpha) / (k + rank + 1)
        if chunk_id in scores:
            scores[chunk_id]["fused_score"] += rrf_score
            scores[chunk_id]["bm25_score"] = result.score
        else:
            scores[chunk_id] = {...}

    # 按融合分数排序
    return sorted(results, key=lambda x: x.fused_score, reverse=True)
```

**2. BM25 实现**

```python
class BM25Service:
    """Okapi BM25 关键词检索"""

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1  # 词频饱和参数
        self.b = b    # 文档长度归一化参数

    def tokenize(self, text: str) -> List[str]:
        """中英文分词 (使用 jieba)"""
        tokens = list(jieba.cut(text.lower()))
        return [t for t in tokens if len(t.strip()) > 1]

    def score(self, query_tokens: List[str], chunk_id: str) -> float:
        """
        计算 BM25 分数

        公式: BM25(D,Q) = Σ IDF(qi) * (f(qi,D) * (k1+1)) / (f(qi,D) + k1 * (1-b+b*|D|/avgdl))
        """
        score = 0.0
        doc_len = self.doc_lengths[chunk_id]

        for term in query_tokens:
            if term not in self.inverted_index:
                continue

            tf = self.inverted_index[term].get(chunk_id, 0)
            df = self.term_doc_freq[term]

            # IDF 计算
            idf = math.log((self.doc_count - df + 0.5) / (df + 0.5) + 1)

            # TF 归一化
            tf_norm = (tf * (self.k1 + 1)) / (
                tf + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len)
            )

            score += idf * tf_norm

        return score
```

**3. 向量存储 (pgvector)**

```python
class PgvectorService:
    """
    PostgreSQL + pgvector 向量数据库服务

    支持两种模式：
    1. PostgreSQL + pgvector: 完整向量搜索功能
    2. SQLite (测试模式): 降级为暴力搜索
    """

    def search(self, query_vector: List[float], top_k: int = 10) -> List[VectorSearchResult]:
        """向量相似度搜索"""
        if self._pgvector_enabled:
            # 使用 pgvector 的 <-> 运算符 (L2 距离)
            sql = """
                SELECT id, document_id, content, page_number, metadata,
                       embedding <-> :query_vector AS distance
                FROM vector_chunks
                WHERE user_id = :user_id
                ORDER BY distance
                LIMIT :top_k
            """
        else:
            # SQLite 降级: 暴力搜索 (余弦相似度)
            # 加载所有向量，在内存中计算
            ...
```

#### 智能分块策略

| 策略 | 适用场景 | 特点 |
|------|----------|------|
| **语义分块** | 通用文档 | 按语义边界分割，保持上下文完整 |
| **页面感知分块** | PDF/学术论文 | 保留页码信息，支持引用定位 |
| **递归分块** | 长文档 | 先按大块分割，再递归细分 |
| **固定大小分块** | 结构化文档 | 简单高效，适合格式统一的文档 |

#### 引用追溯机制

```python
class CitationService:
    """引用追溯服务"""

    def create_citation(self, chunk: Chunk, query: str) -> Citation:
        """创建引用信息"""
        return Citation(
            document_id=chunk.document_id,
            document_title=chunk.document.title,
            page_number=chunk.page_number,
            content_preview=chunk.content[:200],
            relevance_score=chunk.score,
            highlight_positions=self._find_highlights(chunk.content, query)
        )
```

#### 面试问答

**Q: 为什么要用混合检索而不是纯向量检索?**

> 1. **互补性**: 向量检索擅长语义相似，BM25 擅长精确匹配
> 2. **专业术语**: 向量模型可能不认识专业术语，BM25 可以精确匹配
> 3. **鲁棒性**: 单一方法可能失效，混合方法更稳定
> 4. **实测效果**: 混合检索比单一方法准确率提升约 30%

**Q: RRF 算法的原理是什么?**

> RRF (Reciprocal Rank Fusion) 是一种排名融合算法：
> - 公式: `RRF(d) = Σ 1/(k + rank_i)`
> - k 是常数（通常 60），用于平滑排名差异
> - 不依赖原始分数，只看排名位置
> - 对不同检索方法的分数尺度不敏感

**Q: 为什么选择 pgvector 而不是 Milvus?**

> | 特性 | pgvector | Milvus |
> |------|----------|--------|
> | 部署复杂度 | 低（PostgreSQL 扩展） | 高（独立服务） |
> | 运维成本 | 低 | 高 |
> | 性能 | 中等（百万级） | 高（亿级） |
> | 云服务支持 | Supabase 原生支持 | 需要自建 |
>
> 选择 pgvector 的原因：项目规模不大，优先考虑部署简单和运维成本。

**Q: 如何优化 RAG 检索质量?**

> 1. **分块策略**: 根据文档类型选择合适的分块方法
> 2. **Chunk 大小**: 1000-1500 tokens 效果较好
> 3. **Overlap**: 设置 10-20% 的重叠，避免信息断裂
> 4. **Reranker**: 使用 bge-reranker 对结果重排序
> 5. **Query 改写**: 对用户查询进行扩展或改写

---

### 4.3.1 RAG 全流程深度解析 (面试重点)

> **面试官提示**: RAG 是本项目的核心亮点，面试时需要能够完整、流畅地讲解整个流程。

#### 一、RAG 概述与核心价值

**什么是 RAG?**

RAG (Retrieval-Augmented Generation，检索增强生成) 是一种将外部知识库与大语言模型结合的技术架构。它解决了 LLM 的两个核心问题：

1. **知识截止问题**: LLM 训练数据有截止日期，无法获取最新信息
2. **幻觉问题**: LLM 可能生成看似合理但实际错误的内容

**RAG 的核心思想**:
```
用户问题 → 检索相关文档 → 将文档作为上下文 → LLM 基于上下文生成回答
```

**本项目 RAG 的技术亮点**:
- 混合检索 (向量 + BM25)
- RRF 排名融合算法
- Cross-Encoder 重排序
- 智能分块策略
- 引用追溯机制

#### 二、RAG 两大核心流程

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        RAG 系统两大核心流程                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                    流程一: 文档摄入 (Ingest)                      │    │
│  │                                                                 │    │
│  │  PDF/Word/TXT → 解析 → 分块 → 向量化 → 存储到向量库 + BM25索引    │    │
│  │                                                                 │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                    流程二: 检索生成 (Retrieve & Generate)         │    │
│  │                                                                 │    │
│  │  用户查询 → 向量检索 + BM25检索 → RRF融合 → Rerank → LLM生成     │    │
│  │                                                                 │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 三、流程一: 文档摄入 (Ingest Pipeline) 详解

##### 3.1 整体流程图

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        文档摄入流程 (Ingest Pipeline)                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────────────┐  │
│  │ 文档上传  │ →  │ 文档解析  │ →  │ 智能分块  │ →  │ 向量化 + 索引构建 │  │
│  │ PDF/Word │    │ 提取文本  │    │ Chunking │    │ Embedding + BM25 │  │
│  └──────────┘    └──────────┘    └──────────┘    └──────────────────┘  │
│       │               │               │                   │            │
│       ▼               ▼               ▼                   ▼            │
│  验证文件格式     MinerU/PyPDF    语义边界分割      text-embedding-3-small │
│  大小限制        元数据提取       页面感知分块      pgvector 存储         │
│                 页码保留         重叠设置          倒排索引构建          │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

##### 3.2 Step 1: 文档解析

**支持的文档格式**:
- PDF (学术论文、技术文档)
- Word (.docx)
- 纯文本 (.txt, .md)

**PDF 解析方案对比**:

| 方案 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **PyPDF2** | 简单、快速 | 复杂排版效果差 | 简单 PDF |
| **MinerU** | 高精度、保留结构 | 需要额外部署 | 学术论文 |
| **pdfplumber** | 表格提取好 | 速度较慢 | 含表格文档 |

**本项目实现** (来自 `chunking_service.py`):
```python
# 文档解析入口
async def process_document(self, file_path: str, doc_type: str) -> List[Chunk]:
    """
    处理文档，返回分块列表

    Args:
        file_path: 文档路径
        doc_type: 文档类型 (pdf/docx/txt)
    """
    # 1. 根据类型选择解析器
    if doc_type == "pdf":
        pages = await self._parse_pdf(file_path)
    elif doc_type == "docx":
        pages = await self._parse_docx(file_path)
    else:
        pages = await self._parse_text(file_path)

    # 2. 执行分块
    chunks = await self._chunk_pages(pages)

    return chunks
```

##### 3.3 Step 2: 智能分块 (Chunking)

**为什么需要分块?**
1. LLM 上下文窗口有限 (如 8K/32K tokens)
2. 检索需要细粒度匹配
3. 太大的文档无法有效向量化

**本项目支持的 4 种分块策略**:

| 策略 | 代码常量 | 适用场景 | 核心逻辑 |
|------|----------|----------|----------|
| **固定大小** | `FIXED` | 结构化文档 | 按字符数切分 |
| **语义分块** | `SEMANTIC` | 通用文档 | 按句子边界切分 |
| **递归分块** | `RECURSIVE` | 长文档 | 先大块再细分 |
| **页面感知** | `PAGE_AWARE` | PDF/论文 | 保留页码信息 |

**核心代码 - 语义边界检测** (来自 `chunking_service.py:_find_sentence_boundary`):
```python
def _find_sentence_boundary(self, text: str, target_pos: int) -> int:
    """
    在目标位置附近找到最近的句子边界
    避免在句子中间切断，保持语义完整性
    """
    # 中英文句子结束符
    sentence_ends = ['。', '！', '？', '.', '!', '?', '\n\n']

    # 向前搜索最近的句子边界
    search_start = max(0, target_pos - 200)
    search_end = min(len(text), target_pos + 200)
    search_text = text[search_start:search_end]

    best_pos = target_pos
    min_distance = float('inf')

    for end_char in sentence_ends:
        pos = search_text.rfind(end_char, 0, target_pos - search_start)
        if pos != -1:
            actual_pos = search_start + pos + len(end_char)
            distance = abs(actual_pos - target_pos)
            if distance < min_distance:
                min_distance = distance
                best_pos = actual_pos

    return best_pos
```

**分块参数配置**:
```python
# 推荐配置
CHUNK_SIZE = 1000      # 每块约 1000 字符
CHUNK_OVERLAP = 200    # 20% 重叠，避免信息断裂
MIN_CHUNK_SIZE = 100   # 最小块大小，避免过小的块
```

**面试话术 - 为什么设置 Overlap?**
> 设置 20% 的重叠是为了解决"边界信息丢失"问题。假设一个重要概念刚好被切分到两个块的边界，没有重叠的话，两个块都无法完整表达这个概念。通过重叠，确保边界附近的信息在相邻块中都有保留。

##### 3.4 Step 3: 向量化 (Embedding)

**什么是 Embedding?**

将文本转换为高维向量（如 1536 维），使得语义相似的文本在向量空间中距离更近。

**本项目使用的 Embedding 模型**:
- **模型**: `text-embedding-3-small` (OpenAI)
- **维度**: 1536
- **特点**: 性价比高，支持中英文

**核心代码** (来自 `embedding_service.py`):
```python
class EmbeddingService:
    """向量化服务"""

    def __init__(self):
        self.model = "text-embedding-3-small"
        self.dimension = 1536

    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        批量向量化文本

        Args:
            texts: 文本列表
        Returns:
            向量列表，每个向量是 1536 维的浮点数列表
        """
        # 调用 OpenAI Embedding API
        response = await self.client.embeddings.create(
            model=self.model,
            input=texts
        )

        return [item.embedding for item in response.data]

    async def embed_query(self, query: str) -> List[float]:
        """向量化单个查询"""
        embeddings = await self.embed_texts([query])
        return embeddings[0]
```

##### 3.5 Step 4: 存储与索引构建

**双重存储架构**:

```
┌─────────────────────────────────────────────────────────────────┐
│                      存储层架构                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────┐      ┌─────────────────────┐          │
│  │   向量存储 (pgvector)│      │   BM25 倒排索引     │          │
│  │                     │      │                     │          │
│  │  chunk_id → vector  │      │  term → [chunk_ids] │          │
│  │  (1536 维向量)       │      │  (词项到文档映射)    │          │
│  │                     │      │                     │          │
│  │  支持:              │      │  支持:              │          │
│  │  - L2 距离搜索      │      │  - 精确关键词匹配   │          │
│  │  - 余弦相似度       │      │  - TF-IDF 加权      │          │
│  │  - HNSW 索引加速    │      │  - 中文分词 (jieba) │          │
│  └─────────────────────┘      └─────────────────────┘          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**pgvector 存储实现** (来自 `pgvector_service.py`):
```python
class PgvectorService:
    """PostgreSQL + pgvector 向量存储服务"""

    async def add_chunks(self, chunks: List[Chunk], embeddings: List[List[float]]):
        """
        存储分块和对应的向量
        """
        for chunk, embedding in zip(chunks, embeddings):
            await self.db.execute(
                """
                INSERT INTO vector_chunks
                (id, document_id, content, embedding, page_number, metadata, user_id)
                VALUES (:id, :doc_id, :content, :embedding, :page, :meta, :user_id)
                """,
                {
                    "id": chunk.id,
                    "doc_id": chunk.document_id,
                    "content": chunk.content,
                    "embedding": embedding,  # pgvector 自动处理向量类型
                    "page": chunk.page_number,
                    "meta": json.dumps(chunk.metadata),
                    "user_id": chunk.user_id
                }
            )
```

**BM25 索引构建** (来自 `bm25_service.py`):
```python
class BM25Service:
    """BM25 关键词检索服务"""

    def build_index(self, chunks: List[Chunk]):
        """
        构建 BM25 倒排索引
        """
        self.inverted_index = {}  # term -> {chunk_id: tf}
        self.doc_lengths = {}     # chunk_id -> doc_length
        self.term_doc_freq = {}   # term -> df

        for chunk in chunks:
            # 中文分词
            tokens = list(jieba.cut(chunk.content.lower()))
            tokens = [t for t in tokens if len(t.strip()) > 1]

            self.doc_lengths[chunk.id] = len(tokens)

            # 统计词频
            term_freq = Counter(tokens)
            for term, freq in term_freq.items():
                if term not in self.inverted_index:
                    self.inverted_index[term] = {}
                self.inverted_index[term][chunk.id] = freq

                # 更新文档频率
                if term not in self.term_doc_freq:
                    self.term_doc_freq[term] = 0
                self.term_doc_freq[term] += 1

        # 计算平均文档长度
        self.avg_doc_len = sum(self.doc_lengths.values()) / len(self.doc_lengths)
        self.doc_count = len(chunks)
```

#### 四、流程二: 检索生成 (Retrieve Pipeline) 详解

##### 4.1 整体流程图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     检索生成流程 (Retrieve Pipeline)                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  用户查询: "LangGraph 的状态管理机制是什么?"                                   │
│      │                                                                      │
│      ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Step 1: 查询预处理                                │    │
│  │  - 向量化查询 (text-embedding-3-small)                              │    │
│  │  - 中文分词 (jieba): ["langgraph", "状态", "管理", "机制"]           │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│      │                                                                      │
│      ├──────────────────────────────────────────┐                          │
│      ▼                                          ▼                          │
│  ┌──────────────────────┐              ┌──────────────────────┐            │
│  │  Step 2a: 向量检索    │              │  Step 2b: BM25 检索   │            │
│  │  pgvector L2 距离     │              │  关键词精确匹配       │            │
│  │  返回 Top-20         │              │  返回 Top-20         │            │
│  └──────────────────────┘              └──────────────────────┘            │
│      │                                          │                          │
│      └──────────────────────────────────────────┘                          │
│                         │                                                   │
│                         ▼                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Step 3: RRF 排名融合                              │    │
│  │  score(d) = α/(k+rank_vector) + (1-α)/(k+rank_bm25)                │    │
│  │  合并去重，按融合分数排序                                             │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                         │                                                   │
│                         ▼                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Step 4: Reranker 重排序                           │    │
│  │  使用 Cross-Encoder (bge-reranker) 精排 Top-10                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                         │                                                   │
│                         ▼                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Step 5: 构建上下文 + LLM 生成                      │    │
│  │  将 Top-K 结果作为上下文，调用 LLM 生成最终回答                        │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

##### 4.2 Step 2a: 向量检索详解

**向量检索原理**:
1. 将查询文本转换为向量
2. 在向量空间中找到最近邻的文档向量
3. 使用距离度量（L2 距离或余弦相似度）

**pgvector 向量检索** (来自 `pgvector_service.py`):
```python
async def search(
    self,
    query_vector: List[float],
    user_id: str,
    top_k: int = 20
) -> List[VectorSearchResult]:
    """
    向量相似度搜索

    使用 pgvector 的 <-> 运算符计算 L2 距离
    距离越小，相似度越高
    """
    if self._pgvector_enabled:
        # PostgreSQL + pgvector 模式
        results = await self.db.fetch_all(
            """
            SELECT
                id,
                document_id,
                content,
                page_number,
                metadata,
                embedding <-> :query_vector AS distance
            FROM vector_chunks
            WHERE user_id = :user_id
            ORDER BY distance ASC
            LIMIT :top_k
            """,
            {
                "query_vector": str(query_vector),  # pgvector 格式
                "user_id": user_id,
                "top_k": top_k
            }
        )
    else:
        # SQLite 降级模式: 内存中暴力搜索
        all_chunks = await self._load_all_chunks(user_id)
        results = self._brute_force_search(query_vector, all_chunks, top_k)

    return [
        VectorSearchResult(
            id=r["id"],
            content=r["content"],
            score=1 / (1 + r["distance"]),  # 转换为相似度分数
            page_number=r["page_number"]
        )
        for r in results
    ]
```

**面试话术 - L2 距离 vs 余弦相似度**:
> L2 距离（欧氏距离）计算的是向量间的绝对距离，余弦相似度计算的是向量夹角。对于归一化后的向量，两者效果相近。pgvector 默认使用 L2 距离，因为计算更快。实际项目中，我们将 L2 距离转换为相似度分数：`score = 1 / (1 + distance)`。

##### 4.3 Step 2b: BM25 检索详解

**BM25 算法原理**:

BM25 (Best Matching 25) 是一种基于概率的排名函数，核心公式：

```
BM25(D, Q) = Σ IDF(qi) × (f(qi,D) × (k1+1)) / (f(qi,D) + k1 × (1-b+b×|D|/avgdl))
```

其中：
- `IDF(qi)`: 逆文档频率，衡量词的稀有程度
- `f(qi,D)`: 词 qi 在文档 D 中的词频
- `k1`: 词频饱和参数（通常 1.2-2.0）
- `b`: 文档长度归一化参数（通常 0.75）
- `|D|`: 文档长度
- `avgdl`: 平均文档长度

**本项目 BM25 实现** (来自 `bm25_service.py`):
```python
def _compute_bm25_score(
    self,
    query_tokens: List[str],
    chunk_id: str
) -> float:
    """
    计算单个文档的 BM25 分数
    """
    score = 0.0
    doc_len = self.doc_lengths[chunk_id]

    for term in query_tokens:
        if term not in self.inverted_index:
            continue

        # 获取词频
        tf = self.inverted_index[term].get(chunk_id, 0)
        if tf == 0:
            continue

        # 计算 IDF
        df = self.term_doc_freq[term]
        idf = math.log((self.doc_count - df + 0.5) / (df + 0.5) + 1)

        # 计算 TF 归一化
        tf_norm = (tf * (self.k1 + 1)) / (
            tf + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len)
        )

        score += idf * tf_norm

    return score

def search(self, query: str, top_k: int = 20) -> List[BM25Result]:
    """BM25 检索"""
    # 1. 查询分词
    query_tokens = self.tokenize(query)

    # 2. 计算所有文档的 BM25 分数
    scores = []
    for chunk_id in self.doc_lengths.keys():
        score = self._compute_bm25_score(query_tokens, chunk_id)
        if score > 0:
            scores.append((chunk_id, score))

    # 3. 排序返回 Top-K
    scores.sort(key=lambda x: x[1], reverse=True)
    return [
        BM25Result(chunk_id=cid, score=s)
        for cid, s in scores[:top_k]
    ]
```

**面试话术 - 为什么 BM25 对专业术语效果好?**
> BM25 是精确匹配算法，只要文档包含查询词就会有分数。而向量检索依赖 Embedding 模型的语义理解，如果模型没见过某个专业术语（如 "LangGraph"），可能无法正确编码其语义。BM25 不依赖语义理解，只要词匹配就能检索到，所以对专业术语更可靠。

##### 4.4 Step 3: RRF 排名融合详解

**为什么需要融合?**

| 检索方法 | 优势 | 劣势 |
|----------|------|------|
| 向量检索 | 语义理解强，能找到同义表达 | 对专业术语可能失效 |
| BM25 | 精确匹配，对专业术语可靠 | 无法理解语义，同义词检索差 |

**RRF 算法原理**:

RRF (Reciprocal Rank Fusion) 是一种基于排名的融合算法：

```
RRF(d) = Σ 1/(k + rank_i(d))
```

- 只看排名位置，不看原始分数
- k 是平滑常数（通常 60），防止排名第一的权重过大
- 对不同检索方法的分数尺度不敏感

**本项目 RRF 实现** (来自 `search_service.py`):
```python
def rrf_fusion(
    self,
    vector_results: List[VectorSearchResult],
    bm25_results: List[BM25Result],
    alpha: float = 0.5,
    k: int = 60
) -> List[FusedResult]:
    """
    RRF 融合算法

    Args:
        vector_results: 向量检索结果
        bm25_results: BM25 检索结果
        alpha: 向量检索权重 (0-1)
        k: RRF 平滑常数
    """
    scores: Dict[str, Dict] = {}

    # 处理向量检索结果
    for rank, result in enumerate(vector_results):
        chunk_id = result.id
        rrf_score = alpha / (k + rank + 1)
        scores[chunk_id] = {
            "fused_score": rrf_score,
            "vector_rank": rank + 1,
            "vector_score": result.score,
            "content": result.content,
            "page_number": result.page_number
        }

    # 处理 BM25 检索结果
    for rank, result in enumerate(bm25_results):
        chunk_id = result.chunk_id
        rrf_score = (1 - alpha) / (k + rank + 1)

        if chunk_id in scores:
            # 两种方法都检索到，累加分数
            scores[chunk_id]["fused_score"] += rrf_score
            scores[chunk_id]["bm25_rank"] = rank + 1
            scores[chunk_id]["bm25_score"] = result.score
        else:
            # 只有 BM25 检索到
            scores[chunk_id] = {
                "fused_score": rrf_score,
                "bm25_rank": rank + 1,
                "bm25_score": result.score,
                ...
            }

    # 按融合分数排序
    fused_results = sorted(
        scores.values(),
        key=lambda x: x["fused_score"],
        reverse=True
    )

    return fused_results
```

**RRF 计算示例**:
```
假设 k=60, alpha=0.5

文档 A: 向量排名第1, BM25排名第3
  RRF(A) = 0.5/(60+1) + 0.5/(60+3) = 0.00820 + 0.00794 = 0.01614

文档 B: 向量排名第5, BM25排名第1
  RRF(B) = 0.5/(60+5) + 0.5/(60+1) = 0.00769 + 0.00820 = 0.01589

文档 C: 只有向量排名第2
  RRF(C) = 0.5/(60+2) + 0 = 0.00806

结果: A > B > C
```

##### 4.5 Step 4: Reranker 重排序详解

**为什么需要 Reranker?**

| 阶段 | 方法 | 特点 |
|------|------|------|
| 召回 (Recall) | 向量检索 + BM25 | 快速，但精度有限 |
| 精排 (Rerank) | Cross-Encoder | 慢，但精度高 |

**Bi-Encoder vs Cross-Encoder**:

```
Bi-Encoder (向量检索):
  Query → Encoder → Query Vector  ─┐
                                   ├→ 余弦相似度
  Doc   → Encoder → Doc Vector   ─┘

  优点: 文档向量可预计算，检索快
  缺点: Query 和 Doc 独立编码，交互信息少

Cross-Encoder (Reranker):
  [Query, Doc] → Encoder → Relevance Score

  优点: Query 和 Doc 联合编码，精度高
  缺点: 每次都要重新计算，速度慢
```

**本项目 Reranker 实现** (来自 `rerank_service.py`):
```python
class RerankService:
    """Cross-Encoder 重排序服务"""

    def __init__(self):
        # 使用 BGE-Reranker 模型
        self.model_name = "BAAI/bge-reranker-base"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name
        )

    def rerank(
        self,
        query: str,
        candidates: List[FusedResult],
        top_k: int = 5
    ) -> List[RerankResult]:
        """
        对候选结果重排序

        Args:
            query: 用户查询
            candidates: RRF 融合后的候选结果
            top_k: 返回前 K 个结果
        """
        # 构建 (query, doc) 对
        pairs = [(query, c.content) for c in candidates]

        # 批量计算相关性分数
        with torch.no_grad():
            inputs = self.tokenizer(
                pairs,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            scores = self.model(**inputs).logits.squeeze(-1)
            scores = torch.sigmoid(scores).tolist()

        # 按分数排序
        scored_results = [
            RerankResult(
                chunk_id=c.chunk_id,
                content=c.content,
                rerank_score=score,
                original_rank=i + 1
            )
            for i, (c, score) in enumerate(zip(candidates, scores))
        ]

        scored_results.sort(key=lambda x: x.rerank_score, reverse=True)
        return scored_results[:top_k]
```

**面试话术 - 为什么用两阶段检索?**
> 这是经典的"召回-精排"架构。第一阶段用向量检索和 BM25 快速召回大量候选（如 Top-100），第二阶段用 Cross-Encoder 对候选精排。Cross-Encoder 精度高但速度慢，不适合全库检索；Bi-Encoder 速度快但精度有限。两阶段结合，既保证速度又保证精度。

##### 4.6 Step 5: 引用追溯机制

**引用追溯的价值**:
1. 让用户知道答案来源，增强可信度
2. 支持用户查看原文上下文
3. 便于用户验证和深入阅读

**本项目引用追溯实现** (来自 `citation_service.py`):
```python
class CitationService:
    """引用追溯服务"""

    async def get_citation_detail(
        self,
        chunk_id: str,
        query: str
    ) -> CitationDetail:
        """
        获取引用详情，包括上下文

        Args:
            chunk_id: 被引用的分块 ID
            query: 用户查询（用于高亮）
        """
        # 1. 获取当前分块
        chunk = await self.chunk_repo.get_by_id(chunk_id)

        # 2. 获取相邻分块（上下文）
        prev_chunks = await self.chunk_repo.get_prev_chunks(
            document_id=chunk.document_id,
            chunk_index=chunk.chunk_index,
            count=2  # 前 2 个分块
        )
        next_chunks = await self.chunk_repo.get_next_chunks(
            document_id=chunk.document_id,
            chunk_index=chunk.chunk_index,
            count=2  # 后 2 个分块
        )

        # 3. 高亮匹配文本
        highlighted_content = self._highlight_matches(
            chunk.content,
            query
        )

        return CitationDetail(
            document_id=chunk.document_id,
            document_title=chunk.document.title,
            page_number=chunk.page_number,
            content=chunk.content,
            highlighted_content=highlighted_content,
            prev_context=[c.content for c in prev_chunks],
            next_context=[c.content for c in next_chunks],
            source_url=chunk.document.source_url
        )

    def _highlight_matches(self, content: str, query: str) -> str:
        """高亮查询词"""
        tokens = jieba.cut(query)
        for token in tokens:
            if len(token) > 1:
                content = content.replace(
                    token,
                    f"<mark>{token}</mark>"
                )
        return content
```

#### 五、RAG 性能优化策略

##### 5.1 检索质量优化

| 优化点 | 方法 | 效果 |
|--------|------|------|
| 分块大小 | 1000-1500 字符 | 平衡粒度和上下文 |
| 分块重叠 | 20% overlap | 避免边界信息丢失 |
| 混合检索 | 向量 + BM25 | 准确率提升 30% |
| Reranker | Cross-Encoder | 精度进一步提升 |
| Query 改写 | LLM 扩展查询 | 提高召回率 |

##### 5.2 性能优化

| 优化点 | 方法 | 效果 |
|--------|------|------|
| 向量索引 | HNSW (pgvector) | 毫秒级检索 |
| 批量处理 | 批量 Embedding | 减少 API 调用 |
| 缓存 | 查询结果缓存 | 重复查询加速 |
| 异步处理 | asyncio | 并发检索 |

#### 六、RAG 面试深度问答

**Q1: 完整描述一下你的 RAG 系统是如何工作的?**

> 我的 RAG 系统分为两个核心流程：
>
> **文档摄入流程**：用户上传文档后，系统首先解析文档提取文本，然后使用智能分块策略将文档切分为 1000 字符左右的块，每块有 20% 重叠。分块后，调用 OpenAI 的 text-embedding-3-small 模型将每个块向量化，存入 pgvector。同时构建 BM25 倒排索引用于关键词检索。
>
> **检索生成流程**：用户提问时，系统并行执行向量检索和 BM25 检索，各返回 Top-20 结果。然后用 RRF 算法融合两路结果，再用 BGE-Reranker 对 Top-20 精排，取 Top-5 作为上下文传给 LLM 生成最终回答。

**Q2: 为什么选择混合检索而不是纯向量检索?**

> 纯向量检索有两个问题：
> 1. **专业术语问题**：Embedding 模型可能没见过某些专业术语，无法正确编码语义
> 2. **精确匹配问题**：用户搜索特定关键词时，向量检索可能返回语义相似但不包含关键词的结果
>
> BM25 是精确匹配，能弥补这两个缺陷。实测混合检索比纯向量检索准确率提升约 30%。

**Q3: RRF 算法相比其他融合方法有什么优势?**

> RRF 的核心优势是**对分数尺度不敏感**。向量检索返回的是相似度分数（0-1），BM25 返回的是 TF-IDF 分数（可能很大），两者尺度完全不同，直接加权平均效果差。
>
> RRF 只看排名位置，不看原始分数，所以不需要归一化，实现简单且效果稳定。公式 `1/(k+rank)` 中的 k=60 是经验值，能平滑排名差异。

**Q4: Cross-Encoder Reranker 的原理是什么?**

> Cross-Encoder 将 Query 和 Document 拼接后一起输入 Transformer，让模型在编码时就能看到两者的交互信息。相比 Bi-Encoder（分别编码再计算相似度），Cross-Encoder 能捕捉更细粒度的语义关系。
>
> 缺点是每个 (Query, Doc) 对都要重新计算，不能预计算文档向量，所以只适合对少量候选精排，不适合全库检索。

**Q5: 如何评估 RAG 系统的效果?**

> 主要从三个维度评估：
> 1. **检索质量**：Recall@K（召回率）、MRR（平均倒数排名）、NDCG（归一化折损累积增益）
> 2. **生成质量**：Faithfulness（忠实度，答案是否基于检索内容）、Relevance（相关性）
> 3. **端到端效果**：人工评估、A/B 测试
>
> 我在项目中主要通过人工构建测试集，评估检索的 Recall@5 和生成的 Faithfulness。

**Q6: 遇到过什么 RAG 相关的难题?如何解决的?**

> 最大的难题是**专业术语检索效果差**。一开始用纯向量检索，发现搜索 "LangGraph" 这类专业词时，返回的结果经常不包含这个词。
>
> 解决方案：
> 1. 引入 BM25 混合检索，确保精确匹配
> 2. 使用 jieba 分词处理中文
> 3. 添加 Reranker 进一步提升精度
>
> 优化后，专业术语的检索准确率从 60% 提升到 90%+。

### 4.4 语音服务 (whisper-service)

#### 架构设计

```
whisper-service/
├── app/
│   ├── api/v1/
│   │   └── voice.py           # 语音 API (转录/合成/语音列表)
│   ├── core/
│   │   └── voice_manager.py   # Whisper + TTS 管理器
│   ├── schemas/
│   │   └── __init__.py        # Pydantic Schema
│   └── config.py              # 配置管理
```

#### 核心功能

| API 端点 | 方法 | 功能 | 说明 |
|----------|------|------|------|
| `/voice/transcribe` | POST | 语音转文字 (STT) | 支持 wav/mp3/webm/ogg |
| `/voice/synthesize` | POST | 文字转语音 (TTS) | 返回 MP3 音频流 |
| `/voice/voices` | GET | 获取可用语音列表 | 支持按语言筛选 |
| `/voice/health` | GET | 健康检查 | 检查模型加载状态 |

#### 核心代码解析

**1. Whisper 语音识别管理器**

```python
class WhisperManager:
    """Whisper 语音识别管理器 - 懒加载模式"""

    def __init__(self):
        self._model: Optional[WhisperModel] = None
        self._lock = asyncio.Lock()  # 防止并发加载
        self._loading = False

    async def get_model(self, model_size: str = "base") -> WhisperModel:
        """获取 Whisper 模型（懒加载）"""
        async with self._lock:
            if self._model is not None:
                return self._model

            # 在线程池中加载模型，避免阻塞事件循环
            loop = asyncio.get_event_loop()
            self._model = await loop.run_in_executor(
                None,
                lambda: WhisperModel(
                    model_size,
                    device="cpu",           # 或 "cuda"
                    compute_type="int8"     # 量化类型
                )
            )
            return self._model

    async def transcribe(self, audio_data: bytes, language: str = "auto"):
        """转录音频"""
        model = await self.get_model()

        def _transcribe():
            audio_file = io.BytesIO(audio_data)
            segments, info = model.transcribe(
                audio_file,
                language=None if language == "auto" else language,
                beam_size=5,
                vad_filter=True,  # 语音活动检测
            )
            text = "".join([seg.text for seg in segments])
            return text, info.language, info.duration

        # 在线程池中执行，避免阻塞
        return await asyncio.get_event_loop().run_in_executor(None, _transcribe)
```

**2. Edge TTS 语音合成管理器**

```python
class TTSManager:
    """Edge TTS 语音合成管理器"""

    # 预定义的语音列表
    AVAILABLE_VOICES = {
        "zh-CN-XiaoxiaoNeural": {"name": "晓晓 (女)", "lang": "zh-CN"},
        "zh-CN-YunxiNeural": {"name": "云希 (男)", "lang": "zh-CN"},
        "en-US-JennyNeural": {"name": "Jenny (女)", "lang": "en-US"},
        "ja-JP-NanamiNeural": {"name": "Nanami (女)", "lang": "ja-JP"},
        # ... 更多语音
    }

    async def synthesize(
        self,
        text: str,
        voice: str = "zh-CN-XiaoxiaoNeural",
        rate: str = "+0%",
        pitch: str = "+0Hz"
    ) -> bytes:
        """文字转语音"""
        def _synthesize():
            communicate = edge_tts.Communicate(
                text=text,
                voice=voice,
                rate=rate,
                pitch=pitch
            )
            # 使用同步流式接口收集音频数据
            audio_chunks = []
            for chunk in communicate.stream_sync():
                if chunk["type"] == "audio":
                    audio_chunks.append(chunk["data"])
            return b"".join(audio_chunks)

        return await asyncio.get_event_loop().run_in_executor(None, _synthesize)
```

**3. API 端点实现**

```python
@router.post("/transcribe", response_model=TranscribeResponse)
async def transcribe_audio(
    file: UploadFile = File(...),
    language: str = Form(default="auto"),
    model_size: str = Form(default="base")
):
    """语音转文字"""
    content = await file.read()

    # 验证文件大小和格式
    if len(content) > MAX_UPLOAD_SIZE:
        raise HTTPException(status_code=413, detail="文件过大")

    text, detected_lang, duration = await whisper_manager.transcribe(
        audio_data=content,
        language=language,
        model_size=model_size
    )

    return TranscribeResponse(
        text=text.strip(),
        language=detected_lang,
        duration=duration
    )

@router.post("/synthesize")
async def synthesize_speech(request: TTSCreateRequest):
    """文字转语音"""
    audio_data = await tts_manager.synthesize(
        text=request.text,
        voice=request.voice,
        rate=request.rate
    )

    return StreamingResponse(
        io.BytesIO(audio_data),
        media_type="audio/mpeg"
    )
```

#### 技术要点

| 技术点 | 实现方式 | 优势 |
|--------|----------|------|
| **懒加载** | 首次请求时才加载模型 | 减少启动时间和内存占用 |
| **异步执行** | `run_in_executor` 包装同步代码 | 不阻塞事件循环 |
| **并发控制** | `asyncio.Lock` 防止重复加载 | 避免资源浪费 |
| **VAD 过滤** | 语音活动检测 | 过滤静音，提高准确率 |
| **流式返回** | `StreamingResponse` | 大文件不占用内存 |

#### 面试问答

**Q: 为什么选择 faster-whisper 而不是原版 Whisper?**

> faster-whisper 使用 CTranslate2 优化，相比原版：
> 1. **速度快 4 倍**: 使用 CTranslate2 推理引擎
> 2. **内存占用减少 50%**: 支持 int8 量化
> 3. **支持 CPU 和 GPU**: 自动选择最优设备
> 4. **API 兼容原版**: 迁移成本低

**Q: 为什么选择 Edge TTS 而不是其他 TTS 方案?**

> | 方案 | 优点 | 缺点 |
> |------|------|------|
> | **Edge TTS** | 免费、高质量、多语言 | 依赖网络 |
> | Google TTS | 质量高 | 需要 API Key，收费 |
> | pyttsx3 | 本地运行 | 音质较差 |
> | Azure TTS | 企业级质量 | 收费较高 |
>
> 选择 Edge TTS 的原因：免费、音质好、支持中文多种音色。

**Q: 如何处理大音频文件?**

> 1. **文件大小限制**: 设置 MAX_UPLOAD_SIZE (如 50MB)
> 2. **流式处理**: 使用 `StreamingResponse` 返回音频
> 3. **异步执行**: 在线程池中处理，不阻塞主线程
> 4. **VAD 过滤**: 自动跳过静音部分，加快处理

**Q: 如何实现模型的懒加载?**

> ```python
> # 使用单例模式 + 锁机制
> async def get_model(self):
>     async with self._lock:  # 防止并发加载
>         if self._model is None:
>             self._model = await load_model()
>         return self._model
> ```
> 优势：首次请求才加载，减少启动时间；锁机制防止重复加载。

---

## 五、技术亮点与难点

### 5.1 RAG 混合检索优化

**问题**: 纯向量检索对专业术语效果差

**解决方案**:
1. 引入 BM25 关键词检索，精确匹配专业术语
2. 使用 RRF 算法融合两种检索结果
3. 添加 Reranker 重排序，进一步提升准确率

**效果**: 检索准确率提升约 30%

### 5.2 SSE 流式响应设计

**问题**: 传统 HTTP 请求无法实时显示 AI 生成过程

**解决方案**:
```python
# 后端: FastAPI StreamingResponse
async def chat_stream():
    async for event in agent.astream_events(...):
        yield f"event: {event_type}\ndata: {data}\n\n"

# 前端: EventSource 接收
const eventSource = new EventSource(url);
eventSource.addEventListener('text', (e) => {
    // 实时显示文本
});
eventSource.addEventListener('tool_start', (e) => {
    // 显示工具调用状态
});
```

**效果**: 用户可以实时看到 AI 思考过程和工具调用

### 5.3 Agent 工具系统设计

**问题**: 需要支持大量工具，且要易于扩展

**解决方案**:
1. 使用 MCP (Model Context Protocol) 协议集成外部工具
2. 自定义工具使用 LangChain Tool 装饰器
3. 工具按类型分组，动态加载

**效果**: 支持 96+ 工具，新增工具只需添加配置

### 5.4 代码执行沙箱集成

**问题**: 需要安全执行用户代码，且支持数据可视化

**解决方案**:
1. 使用 E2B 云沙箱，完全隔离执行环境
2. 支持 matplotlib 图表生成，返回 base64 图片
3. 设置超时和资源限制

**效果**: 安全执行 Python 代码，支持图表生成

---

## 六、面试常见问题与回答

### 6.1 项目相关问题

**Q: 这个项目解决了什么问题?**

> 现有的 AI 工具要么功能单一（只能聊天），要么不支持本地知识库。我的项目集成了多工具 Agent、RAG 知识库、代码执行等功能，为研究人员提供一站式的 AI 助理。

**Q: 项目中最有挑战的部分是什么?**

> RAG 检索质量优化。一开始用纯向量检索，发现对专业术语效果很差。后来我研究了混合检索方案，引入 BM25 和 Reranker，并优化了分块策略，最终达到了满意的效果。

**Q: 如果让你重新做这个项目，会有什么改进?**

> 1. 更早引入测试驱动开发
> 2. 考虑使用 GraphQL 替代 REST API
> 3. 添加更完善的监控和日志系统

### 6.2 技术深度问题

**Q: 解释一下 LangGraph 的工作原理**

> LangGraph 是基于图的 Agent 框架：
> 1. **节点 (Node)**: 代表处理步骤，如 LLM 调用、工具执行
> 2. **边 (Edge)**: 定义节点间的流转逻辑
> 3. **状态 (State)**: 在节点间传递的数据
> 4. **Checkpointer**: 持久化状态，支持会话恢复
>
> ReAct Agent 的图结构：
> ```
> Input → LLM Decision → Tool Execution → Output
>              ↑              │
>              └──────────────┘ (循环直到完成)
> ```

**Q: 向量检索的原理是什么?**

> 1. **Embedding**: 将文本转换为高维向量（如 1536 维）
> 2. **相似度计算**: 使用余弦相似度或 L2 距离
> 3. **索引加速**: 使用 HNSW、IVF 等索引结构
> 4. **Top-K 检索**: 返回最相似的 K 个结果

**Q: JWT 的安全性如何保证?**

> 1. **签名验证**: 使用 HS256/RS256 算法签名
> 2. **过期时间**: Access Token 短有效期
> 3. **HTTPS**: 传输层加密
> 4. **HttpOnly Cookie**: 防止 XSS 攻击
> 5. **Token 黑名单**: 支持主动撤销

### 6.3 设计决策问题

**Q: 为什么采用微服务架构?**

> 1. **独立部署**: 各服务可以独立扩展和部署
> 2. **技术异构**: 不同服务可以用不同技术栈
> 3. **故障隔离**: 一个服务故障不影响其他服务
> 4. **团队协作**: 不同团队可以负责不同服务

**Q: 为什么选择 FastAPI?**

> 1. **性能**: 基于 Starlette，性能接近 Node.js
> 2. **类型安全**: 原生支持 Pydantic 类型验证
> 3. **自动文档**: 自动生成 OpenAPI 文档
> 4. **异步支持**: 原生支持 async/await

**Q: 为什么选择 Next.js?**

> 1. **SSR/SSG**: 支持服务端渲染，SEO 友好
> 2. **App Router**: 新的路由系统，更灵活
> 3. **API Routes**: 可以写简单的后端逻辑
> 4. **生态丰富**: shadcn/ui、Tailwind 等

---

## 七、技术选型理由

### 7.1 为什么选择 LangGraph 而不是 LangChain Agent?

| 特性 | LangChain Agent | LangGraph |
|------|-----------------|-----------|
| 状态管理 | 简单 | 完善（Checkpointer） |
| 流式支持 | 有限 | 原生支持 |
| 可扩展性 | 中等 | 高（图结构） |
| 调试 | 困难 | 友好 |

**结论**: LangGraph 更适合复杂的 Agent 场景

### 7.2 为什么选择 pgvector 而不是 Milvus?

| 特性 | pgvector | Milvus |
|------|----------|--------|
| 部署 | 简单（PostgreSQL 扩展） | 复杂（独立服务） |
| 运维 | 低成本 | 高成本 |
| 性能 | 百万级 | 亿级 |
| 云支持 | Supabase 原生 | 需自建 |

**结论**: 项目规模不大，pgvector 足够且更易部署

### 7.3 为什么选择 SSE 而不是 WebSocket?

| 特性 | SSE | WebSocket |
|------|-----|-----------|
| 方向 | 单向 | 双向 |
| 复杂度 | 简单 | 复杂 |
| 重连 | 自动 | 手动 |
| 协议 | HTTP | 独立 |

**结论**: 聊天场景主要是服务器推送，SSE 更简单

### 7.4 为什么采用微服务架构?

**优点**:
1. 独立部署和扩展
2. 技术栈灵活
3. 故障隔离
4. 团队协作

**缺点**:
1. 运维复杂度增加
2. 网络延迟
3. 数据一致性挑战

**结论**: 项目功能模块清晰，微服务架构更合适

---

> 文档最后更新: 2026-01-08
>
> 建议: 面试前重点复习 RAG 混合检索、LangGraph Agent、JWT 认证这三个核心模块
