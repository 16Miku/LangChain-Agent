# Whisper and TTS Service Manager
# è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆæœåŠ¡ç®¡ç†å™¨

import asyncio
import io
import logging
from typing import Optional, Tuple
from faster_whisper import WhisperModel
import edge_tts

from app.config import settings

logger = logging.getLogger(__name__)


class WhisperManager:
    """Whisper è¯­éŸ³è¯†åˆ«ç®¡ç†å™¨"""

    def __init__(self):
        self._model: Optional[WhisperModel] = None
        self._lock = asyncio.Lock()
        self._loading = False

    async def get_model(self, model_size: Optional[str] = None) -> WhisperModel:
        """è·å– Whisper æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
        model_size = model_size or settings.WHISPER_MODEL_SIZE

        async with self._lock:
            # å¦‚æœå·²åŠ è½½ä¸”æ¨¡å‹å¤§å°åŒ¹é…ï¼Œç›´æ¥è¿”å›
            if self._model is not None:
                return self._model

            # é˜²æ­¢é‡å¤åŠ è½½
            if self._loading:
                # ç­‰å¾…åŠ è½½å®Œæˆ
                for _ in range(50):  # æœ€å¤šç­‰5ç§’
                    await asyncio.sleep(0.1)
                    if self._model is not None:
                        return self._model
                raise RuntimeError("Whisper æ¨¡å‹åŠ è½½è¶…æ—¶")

            self._loading = True

            try:
                logger.info(f"ğŸ“¦ æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹: {model_size}...")
                # åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
                loop = asyncio.get_event_loop()
                self._model = await loop.run_in_executor(
                    None,
                    lambda: WhisperModel(
                        model_size,
                        device=settings.WHISPER_DEVICE,
                        compute_type=settings.WHISPER_COMPUTE_TYPE
                    )
                )
                logger.info(f"âœ… Whisper æ¨¡å‹åŠ è½½å®Œæˆ: {model_size}")
                return self._model
            except Exception as e:
                logger.error(f"âŒ Whisper æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                raise
            finally:
                self._loading = False

    async def transcribe(
        self,
        audio_data: bytes,
        language: str = "auto",
        model_size: Optional[str] = None
    ) -> Tuple[str, str, float]:
        """
        è½¬å½•éŸ³é¢‘

        Args:
            audio_data: éŸ³é¢‘æ•°æ® (bytes)
            language: è¯­è¨€ä»£ç  (auto, zh, en, etc.)
            model_size: æ¨¡å‹å¤§å°

        Returns:
            (text, detected_language, duration)
        """
        model = await self.get_model(model_size)

        # å°† language="auto" è½¬æ¢ä¸º None
        lang = None if language == "auto" else language

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè½¬å½•
        loop = asyncio.get_event_loop()

        def _transcribe():
            # å°† bytes è½¬æ¢ä¸º BytesIO å¯¹è±¡ï¼ˆç±»æ–‡ä»¶å¯¹è±¡ï¼‰
            audio_file = io.BytesIO(audio_data)
            segments, info = model.transcribe(
                audio_file,
                language=lang,
                beam_size=5,
                vad_filter=True,  # è¯­éŸ³æ´»åŠ¨æ£€æµ‹è¿‡æ»¤
                vad_parameters=dict(min_silence_duration_ms=500)
            )

            text_parts = []
            for segment in segments:
                text_parts.append(segment.text)

            return "".join(text_parts), info.language, info.duration

        text, detected_lang, duration = await loop.run_in_executor(None, _transcribe)

        logger.info(f"ğŸ¤ è½¬å½•å®Œæˆ: è¯­è¨€={detected_lang}, æ—¶é•¿={duration:.2f}s, æ–‡å­—é•¿åº¦={len(text)}")

        return text, detected_lang, duration


class TTSManager:
    """Edge TTS è¯­éŸ³åˆæˆç®¡ç†å™¨"""

    # å¸¸ç”¨è¯­éŸ³åˆ—è¡¨
    AVAILABLE_VOICES = {
        "zh-CN-XiaoxiaoNeural": {"name": "æ™“æ™“ (å¥³)", "lang": "zh-CN", "gender": "Female"},
        "zh-CN-YunxiNeural": {"name": "äº‘å¸Œ (ç”·)", "lang": "zh-CN", "gender": "Male"},
        "zh-CN-YunyangNeural": {"name": "äº‘æ‰¬ (ç”·)", "lang": "zh-CN", "gender": "Male"},
        "zh-CN-XiaoyiNeural": {"name": "æ™“ä¼Š (å¥³)", "lang": "zh-CN", "gender": "Female"},
        "zh-CN-YunjianNeural": {"name": "äº‘å¥ (ç”·)", "lang": "zh-CN", "gender": "Male"},
        "en-US-JennyNeural": {"name": "Jenny (å¥³)", "lang": "en-US", "gender": "Female"},
        "en-US-GuyNeural": {"name": "Guy (ç”·)", "lang": "en-US", "gender": "Male"},
        "ja-JP-NanamiNeural": {"name": "Nanami (å¥³)", "lang": "ja-JP", "gender": "Female"},
        "ja-JP-KeitaNeural": {"name": "Keita (ç”·)", "lang": "ja-JP", "gender": "Male"},
        "ko-KR-SunHiNeural": {"name": "SunHi (å¥³)", "lang": "ko-KR", "gender": "Female"},
        "ko-KR-InJoonNeural": {"name": "InJoon (ç”·)", "lang": "ko-KR", "gender": "Male"},
    }

    @classmethod
    def get_available_voices(cls, language: Optional[str] = None) -> list[dict]:
        """è·å–å¯ç”¨è¯­éŸ³åˆ—è¡¨"""
        voices = []
        for voice_id, info in cls.AVAILABLE_VOICES.items():
            if language is None or info["lang"] == language:
                voices.append({
                    "id": voice_id,
                    "name": info["name"],
                    "language": info["lang"],
                    "description": f"{info['name']} - {info['lang']}",
                    "gender": info["gender"]
                })
        return voices

    async def synthesize(
        self,
        text: str,
        voice: str = "zh-CN-XiaoxiaoNeural",
        rate: str = "+0%",
        volume: str = "+0%",
        pitch: str = "+0Hz"
    ) -> bytes:
        """
        æ–‡å­—è½¬è¯­éŸ³

        Args:
            text: è¦è½¬æ¢çš„æ–‡å­—
            voice: è¯­éŸ³åç§°
            rate: è¯­é€Ÿ
            volume: éŸ³é‡
            pitch: éŸ³è°ƒ

        Returns:
            MP3 éŸ³é¢‘æ•°æ® (bytes)
        """
        if voice not in self.AVAILABLE_VOICES:
            logger.warning(f"âš ï¸ è¯­éŸ³ {voice} ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯­éŸ³")
            voice = settings.TTS_DEFAULT_VOICE

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ TTS
        loop = asyncio.get_event_loop()

        def _synthesize():
            communicate = edge_tts.Communicate(
                text=text,
                voice=voice,
                rate=rate,
                volume=volume,
                pitch=pitch
            )
            # edge_tts ä½¿ç”¨ save() æ–¹æ³•æˆ–ç”Ÿæˆå™¨è·å–éŸ³é¢‘æ•°æ®
            audio_chunks = []
            for chunk in communicate.stream_sync():
                if chunk["type"] == "audio":
                    audio_chunks.append(chunk["data"])
            return b"".join(audio_chunks)

        audio_data = await loop.run_in_executor(None, _synthesize)

        logger.info(f"ğŸ”Š TTS å®Œæˆ: æ–‡å­—é•¿åº¦={len(text)}, è¯­éŸ³={voice}, å¤§å°={len(audio_data)} bytes")

        return audio_data


# å…¨å±€å•ä¾‹
whisper_manager = WhisperManager()
tts_manager = TTSManager()
